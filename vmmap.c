/******************************************************************************/
/* Important Fall 2025 CSCI 402 usage information:                            */
/*                                                                            */
/* This fils is part of CSCI 402 kernel programming assignments at USC.       */
/*         53616c7465645f5fd1e93dbf35cbffa3aef28f8c01d8cf2ffc51ef62b26a       */
/*         f9bda5a68e5ed8c972b17bab0f42e24b19daa7bd408305b1f7bd6c7208c1       */
/*         0e36230e913039b3046dd5fd0ba706a624d33dbaa4d6aab02c82fe09f561       */
/*         01b0fd977b0051f0b0ce0c69f7db857b1b5e007be2db6d42894bf93de848       */
/*         806d9152bd5715e9                                                   */
/* Please understand that you are NOT permitted to distribute or publically   */
/*         display a copy of this file (or ANY PART of it) for any reason.    */
/* If anyone (including your prospective employer) asks you to post the code, */
/*         you must inform them that you do NOT have permissions to do so.    */
/* You are also NOT permitted to remove or alter this comment block.          */
/* If this comment block is removed or altered in a submitted file, 20 points */
/*         will be deducted.                                                  */
/******************************************************************************/

#include "kernel.h"
#include "errno.h"
#include "globals.h"

#include "vm/vmmap.h"
#include "vm/shadow.h"
#include "vm/anon.h"

#include "proc/proc.h"

#include "util/debug.h"
#include "util/list.h"
#include "util/string.h"
#include "util/printf.h"

#include "fs/vnode.h"
#include "fs/file.h"
#include "fs/fcntl.h"
#include "fs/vfs_syscall.h"

#include "mm/slab.h"
#include "mm/page.h"
#include "mm/mm.h"
#include "mm/mman.h"
#include "mm/mmobj.h"
#include "mm/tlb.h"

static slab_allocator_t *vmmap_allocator;
static slab_allocator_t *vmarea_allocator;

void
vmmap_init(void)
{
        vmmap_allocator = slab_allocator_create("vmmap", sizeof(vmmap_t));
        KASSERT(NULL != vmmap_allocator && "failed to create vmmap allocator!");
        vmarea_allocator = slab_allocator_create("vmarea", sizeof(vmarea_t));
        KASSERT(NULL != vmarea_allocator && "failed to create vmarea allocator!");
}

vmarea_t *
vmarea_alloc(void)
{
        vmarea_t *newvma = (vmarea_t *) slab_obj_alloc(vmarea_allocator);
        if (newvma) {
                newvma->vma_vmmap = NULL;
        }
        return newvma;
}

void
vmarea_free(vmarea_t *vma)
{
        KASSERT(NULL != vma);
        slab_obj_free(vmarea_allocator, vma);
}

/* a debugging routine: dumps the mappings of the given address space. */
size_t
vmmap_mapping_info(const void *vmmap, char *buf, size_t osize)
{
        KASSERT(0 < osize);
        KASSERT(NULL != buf);
        KASSERT(NULL != vmmap);

        vmmap_t *map = (vmmap_t *)vmmap;
        vmarea_t *vma;
        ssize_t size = (ssize_t)osize;

        int len = snprintf(buf, size, "%21s %5s %7s %8s %10s %12s\n",
                           "VADDR RANGE", "PROT", "FLAGS", "MMOBJ", "OFFSET",
                           "VFN RANGE");

        list_iterate_begin(&map->vmm_list, vma, vmarea_t, vma_plink) {
                size -= len;
                buf += len;
                if (0 >= size) {
                        goto end;
                }

                len = snprintf(buf, size,
                               "%#.8x-%#.8x  %c%c%c  %7s 0x%p %#.5x %#.5x-%#.5x\n",
                               vma->vma_start << PAGE_SHIFT,
                               vma->vma_end << PAGE_SHIFT,
                               (vma->vma_prot & PROT_READ ? 'r' : '-'),
                               (vma->vma_prot & PROT_WRITE ? 'w' : '-'),
                               (vma->vma_prot & PROT_EXEC ? 'x' : '-'),
                               (vma->vma_flags & MAP_SHARED ? " SHARED" : "PRIVATE"),
                               vma->vma_obj, vma->vma_off, vma->vma_start, vma->vma_end);
        } list_iterate_end();

end:
        if (size <= 0) {
                size = osize;
                buf[osize - 1] = '\0';
        }
        /*
        KASSERT(0 <= size);
        if (0 == size) {
                size++;
                buf--;
                buf[0] = '\0';
        }
        */
        return osize - size;
}

/* Create a new vmmap, which has no vmareas and does
 * not refer to a process. */
vmmap_t *
vmmap_create(void)
{
/*
    vmmap_t * map = (vmmap_t *)slab_obj_alloc(vmmap_allocator);
    if (map == NULL) {
        return NULL;
    }
    list_init(&(map->vmm_list));
    map->vmm_proc = NULL;
    dbg(DBG_PRINT, "(GRADING3B 1)\n");
    return map;
*/
NOT_YET_IMPLEMENTED("VM:vmmap create");
return NULL;
}

/* Removes all vmareas from the address space and frees the
 * vmmap struct. */
void
vmmap_destroy(vmmap_t *map)
{
/*
    KASSERT(NULL != map);
    dbg(DBG_PRINT, "(GRADING3A 3.a)\n");

    vmarea_t* area = NULL;
    map->vmm_proc = NULL;

    while (!list_empty(&map->vmm_list))
    {
	area = list_item(map->vmm_list.l_next, vmarea_t, vma_plink);
        list_remove(&(area->vma_plink));
        list_remove(&(area->vma_olink));

	//mmobj_t *bottom_obj = mmobj_bottom_obj(area->vma_obj);

        area->vma_obj->mmo_ops->put(area->vma_obj);
        area->vma_obj = NULL;
        vmarea_free(area);
	dbg(DBG_PRINT, "(GRADING3B)\n");
    }

    slab_obj_free(vmmap_allocator, map);
*/
NOT_YET_IMPLEMENTED("VM:vmmap destroy");
}

/* Add a vmarea to an address space. Assumes (i.e. asserts to some extent)
 * the vmarea is valid.  This involves finding where to put it in the list
 * of VM areas, and adding it. Don't forget to set the vma_vmmap for the
 * area. */
void
vmmap_insert(vmmap_t *map, vmarea_t *newvma)
{
/*
    KASSERT(NULL != map && NULL != newvma);
    dbg(DBG_PRINT, "(GRADING3A 3.b)\n");
    KASSERT(NULL == newvma->vma_vmmap);
    dbg(DBG_PRINT, "(GRADING3A 3.b)\n");
    KASSERT(newvma->vma_start < newvma->vma_end);
    dbg(DBG_PRINT, "(GRADING3A 3.b)\n");
    KASSERT(ADDR_TO_PN(USER_MEM_LOW) <= newvma->vma_start && ADDR_TO_PN(USER_MEM_HIGH) >= newvma->vma_end);
    dbg(DBG_PRINT, "(GRADING3A 3.b)\n");

    newvma->vma_vmmap = map;
    vmarea_t* current_area = NULL;

    list_iterate_begin(&map->vmm_list, current_area, vmarea_t, vma_plink) {
        if (current_area->vma_start >= newvma->vma_start) {
            list_insert_before(&current_area->vma_plink, &newvma->vma_plink);
	    dbg(DBG_PRINT, "(GRADING3B)\n");
            return;
        }
    } list_iterate_end();

    list_insert_tail(&map->vmm_list, &newvma->vma_plink);
    dbg(DBG_PRINT, "(GRADING3B)\n");
*/
NOT_YET_IMPLEMENTED("VM:vmmap insert");
}

/* Find a contiguous range of free virtual pages of length npages in
 * the given address space. Returns starting vfn for the range,
 * without altering the map. Returns -1 if no such range exists.
 *
 * Your algorithm should be first fit. If dir is VMMAP_DIR_HILO, you
 * should find a gap as high in the address space as possible; if dir
 * is VMMAP_DIR_LOHI, the gap should be as low as possible. */
int
vmmap_find_range(vmmap_t *map, uint32_t npages, int dir)
{
/*
if (dir == VMMAP_DIR_HILO) {
        vmarea_t *current = NULL;
        vmarea_t *next_area = NULL;
        uint32_t upper_bound = ADDR_TO_PN(USER_MEM_HIGH);
	dbg(DBG_PRINT, "(GRADING3B)\n");

        list_iterate_reverse(&map->vmm_list, current, vmarea_t, vma_plink) {
            if (next_area == NULL) {
                upper_bound = ADDR_TO_PN(USER_MEM_HIGH);
            } else {
                upper_bound = next_area->vma_start;
            }

            if (upper_bound - current->vma_end >= npages) {
                dbg(DBG_PRINT, "(GRADING3C)\n");
                return upper_bound - npages;
            }
            
            next_area = current;
	    dbg(DBG_PRINT, "(GRADING3C)\n");
        } list_iterate_end();

        if (next_area != NULL) {
            upper_bound = next_area->vma_start;
        } else {
            upper_bound = ADDR_TO_PN(USER_MEM_HIGH);
        }

        if (upper_bound - ADDR_TO_PN(USER_MEM_LOW) >= npages) {
            dbg(DBG_PRINT, "(GRADING3C)\n");
            return upper_bound - npages;
        }
    }
    
    return -1;
*/
NOT_YET_IMPLEMENTED("VM:vmmap findrange");
return 0;
}

/* Find the vm_area that vfn lies in. Simply scan the address space
 * looking for a vma whose range covers vfn. If the page is unmapped,
 * return NULL. */
vmarea_t *
vmmap_lookup(vmmap_t *map, uint32_t vfn)
{
/*
    KASSERT(NULL != map);
    dbg(DBG_PRINT, "(GRADING3A 3.c)\n");
    vmarea_t* area = NULL;

    list_iterate_begin(&map->vmm_list, area, vmarea_t, vma_plink) {
        if (vfn >= area->vma_start && vfn < area->vma_end)
        {
            dbg(DBG_PRINT, "(GRADING3B)\n");
            return area;
        }
    } list_iterate_end();

    dbg(DBG_PRINT, "(GRADING3C)\n");
    return NULL;
*/
NOT_YET_IMPLEMENTED("VM:vmmap lookup");
return NULL;
}

/* Allocates a new vmmap containing a new vmarea for each area in the
 * given map. The areas should have no mmobjs set yet. Returns pointer
 * to the new vmmap on success, NULL on failure. This function is
 * called when implementing fork(2). */
vmmap_t *
vmmap_clone(vmmap_t *map)
{
/*
vmmap_t* new_map = vmmap_create();
    vmarea_t* source_area = NULL;

    list_iterate_begin(&map->vmm_list, source_area, vmarea_t, vma_plink) {
        vmarea_t* clone_area = vmarea_alloc();
        
        clone_area->vma_start = source_area->vma_start;
        clone_area->vma_end = source_area->vma_end;
        clone_area->vma_off = source_area->vma_off;
        clone_area->vma_prot = source_area->vma_prot;
        clone_area->vma_flags = source_area->vma_flags;
        clone_area->vma_vmmap = new_map;

        list_link_init(&clone_area->vma_plink);
        list_link_init(&clone_area->vma_olink);

        list_insert_tail(&new_map->vmm_list, &clone_area->vma_plink);
	dbg(DBG_PRINT, "(GRADING3B)\n");
    } list_iterate_end();
    
    dbg(DBG_PRINT, "(GRADING3B)\n");
    return new_map;
*/
NOT_YET_IMPLEMENTED("VM:vmmap clone");
return NULL;
}

/* Insert a mapping into the map starting at lopage for npages pages.
 * If lopage is zero, we will find a range of virtual addresses in the
 * process that is big enough, by using vmmap_find_range with the same
 * dir argument.  If lopage is non-zero and the specified region
 * contains another mapping that mapping should be unmapped.
 *
 * If file is NULL an anon mmobj will be used to create a mapping
 * of 0's.  If file is non-null that vnode's file will be mapped in
 * for the given range.  Use the vnode's mmap operation to get the
 * mmobj for the file; do not assume it is file->vn_obj. Make sure all
 * of the area's fields except for vma_obj have been set before
 * calling mmap.
 *
 * If MAP_PRIVATE is specified set up a shadow object for the mmobj.
 *
 * All of the input to this function should be valid (KASSERT!).
 * See mmap(2) for for description of legal input.
 * Note that off should be page aligned.
 *
 * Be very careful about the order operations are performed in here. Some
 * operation are impossible to undo and should be saved until there
 * is no chance of failure.
 *
 * If 'new' is non-NULL a pointer to the new vmarea_t should be stored in it.
 */
int
vmmap_map(vmmap_t *map, vnode_t *file, uint32_t lopage, uint32_t npages,
          int prot, int flags, off_t off, int dir, vmarea_t **new)
{
/*
KASSERT(NULL != map);
    KASSERT(0 < npages);
    dbg(DBG_PRINT, "(GRADING3A 3.d)\n");
    KASSERT((MAP_SHARED & flags) || (MAP_PRIVATE & flags));
    dbg(DBG_PRINT, "(GRADING3A 3.d)\n");
    KASSERT((0 == lopage) || (ADDR_TO_PN(USER_MEM_LOW) <= lopage));
    dbg(DBG_PRINT, "(GRADING3A 3.d)\n");
    KASSERT((0 == lopage) || (ADDR_TO_PN(USER_MEM_HIGH) >= (lopage + npages)));
    dbg(DBG_PRINT, "(GRADING3A 3.d)\n");
    KASSERT(PAGE_ALIGNED(off));
    dbg(DBG_PRINT, "(GRADING3A 3.d)\n");

    vmarea_t *new_area = vmarea_alloc();
    
    if (lopage == 0) {
        int start_vfn = vmmap_find_range(map, npages, dir);
        if (start_vfn == -1)
        {
            dbg(DBG_PRINT, "(GRADING3C)\n");
            return -1;
        }
        dbg(DBG_PRINT, "(GRADING3B)\n");
        lopage = start_vfn;
    }
    else {
        if (!vmmap_is_range_empty(map, lopage, npages)) {
            dbg(DBG_PRINT, "(GRADING3B)\n");
            vmmap_remove(map, lopage, npages);
        }
        else {
            dbg(DBG_PRINT, "(GRADING3B)\n");
        }
    }

    new_area->vma_start = lopage;
    new_area->vma_end = lopage + npages;
    new_area->vma_off = ADDR_TO_PN(off);
    //new_area->vma_off = off;
    new_area->vma_prot = prot;
    new_area->vma_flags = flags;
    new_area->vma_obj = NULL;
    list_link_init(&new_area->vma_plink);
    list_link_init(&new_area->vma_olink);

    mmobj_t* obj_to_map = NULL;

    if (file == NULL)
    {
        obj_to_map = anon_create();
	dbg(DBG_PRINT, "(GRADING3B)\n");
    } else {
        int retval = file->vn_ops->mmap(file, new_area, &obj_to_map);
        dbg(DBG_PRINT, "(GRADING3B)\n");
    }

    if (flags & MAP_PRIVATE)
    {
        mmobj_t * shadow_obj = shadow_create();
        shadow_obj->mmo_shadowed = obj_to_map;
        new_area->vma_obj = shadow_obj;
        
        mmobj_t *real_obj = obj_to_map;
        shadow_obj->mmo_un.mmo_bottom_obj = real_obj;
        list_insert_head(&(real_obj->mmo_un.mmo_vmas), &new_area->vma_olink);
	dbg(DBG_PRINT, "(GRADING3B)\n");
    }
    else {
        new_area->vma_obj = obj_to_map;
        list_insert_head(&(obj_to_map->mmo_un.mmo_vmas), &new_area->vma_olink);
	dbg(DBG_PRINT, "(GRADING3C)\n");
    }
    
    vmmap_insert(map, new_area);
    
    if (new)
    {
        *new = new_area;
	dbg(DBG_PRINT, "(GRADING3B)\n");
    } else {
        dbg(DBG_PRINT, "(GRADING3B)\n");
    }
    return 0;
*/
NOT_YET_IMPLEMENTED("VM:vmmap vmmap");
return 0;
}

/*
 * We have no guarantee that the region of the address space being
 * unmapped will play nicely with our list of vmareas.
 *
 * You must iterate over each vmarea that is partially or wholly covered
 * by the address range [addr ... addr+len). The vm-area will fall into one
 * of four cases, as illustrated below:
 *
 * key:
 *          [             ]   Existing VM Area
 *        *******             Region to be unmapped
 *
 * Case 1:  [   ******    ]
 * The region to be unmapped lies completely inside the vmarea. We need to
 * split the old vmarea into two vmareas. be sure to increment the
 * reference count to the file associated with the vmarea.
 *
 * Case 2:  [      *******]**
 * The region overlaps the end of the vmarea. Just shorten the length of
 * the mapping.
 *
 * Case 3: *[*****        ]
 * The region overlaps the beginning of the vmarea. Move the beginning of
 * the mapping (remember to update vma_off), and shorten its length.
 *
 * Case 4: *[*************]**
 * The region completely contains the vmarea. Remove the vmarea from the
 * list.
 */
int
vmmap_remove(vmmap_t *map, uint32_t lopage, uint32_t npages)
{
/*
uint32_t unmap_end = lopage + npages;

    list_link_t *current_link = map->vmm_list.l_next;
    list_link_t *next_link = NULL;

    while (current_link != &map->vmm_list) {
        
        vmarea_t *current_vma = list_item(current_link, vmarea_t, vma_plink);
        
        next_link = current_link->l_next;

        uint32_t vma_start = current_vma->vma_start;
        uint32_t vma_end = current_vma->vma_end;

        if (vma_start < lopage && vma_end > unmap_end) {
            dbg(DBG_PRINT, "(GRADING3C)\n");
            
            vmarea_t* new_vma_back = vmarea_alloc();
            new_vma_back->vma_start = unmap_end;
            new_vma_back->vma_end = vma_end;
            new_vma_back->vma_off = current_vma->vma_off + (unmap_end - vma_start);
            new_vma_back->vma_prot = current_vma->vma_prot;
            new_vma_back->vma_flags = current_vma->vma_flags;
            new_vma_back->vma_vmmap = map;

            mmobj_t *top_obj = current_vma->vma_obj;
            new_vma_back->vma_obj = top_obj;

            top_obj->mmo_ops->ref(top_obj);

            current_vma->vma_end = lopage;
            
            list_insert_before(next_link, &new_vma_back->vma_plink);

            mmobj_t *bottom_obj = mmobj_bottom_obj(top_obj);
            list_insert_tail(mmobj_bottom_vmas(bottom_obj), &new_vma_back->vma_olink);
        }
        else if (vma_start < lopage && vma_end > lopage && vma_end <= unmap_end) {
            dbg(DBG_PRINT, "(GRADING3C)\n");
            current_vma->vma_end = lopage;
        }
        else if (vma_start >= lopage && vma_start < unmap_end && vma_end > unmap_end) {
            dbg(DBG_PRINT, "(GRADING3C)\n");
            current_vma->vma_off += (unmap_end - vma_start);
            current_vma->vma_start = unmap_end;
        }
        else if (vma_start >= lopage && vma_end <= unmap_end) {
            dbg(DBG_PRINT, "(GRADING3B)\n");
            
            list_remove(&current_vma->vma_plink);
            list_remove(&current_vma->vma_olink);
            current_vma->vma_obj->mmo_ops->put(current_vma->vma_obj);
            current_vma->vma_obj = NULL;
            vmarea_free(current_vma);
        }
        else {
            dbg(DBG_PRINT, "(GRADING3B)\n");
        }

        current_link = next_link;

    }

    dbg(DBG_PRINT, "(GRADING3B)\n");
    tlb_flush_all();
    proc_t *owner = map->vmm_proc;
if (owner && owner->p_pagedir) {
    pt_unmap_range(owner->p_pagedir,
                   (uintptr_t)PN_TO_ADDR(lopage),
                   (uintptr_t)PN_TO_ADDR(lopage + npages));
}
    return 0;
*/
NOT_YET_IMPLEMENTED("VM:vmmap remove");
return 0;
}

/*
 * Returns 1 if the given address space has no mappings for the
 * given range, 0 otherwise.
 */
int
vmmap_is_range_empty(vmmap_t *map, uint32_t startvfn, uint32_t npages)
{
/*
KASSERT((startvfn < startvfn + npages) && (ADDR_TO_PN(USER_MEM_LOW) <= startvfn) && (ADDR_TO_PN(USER_MEM_HIGH) >= startvfn + npages));
    dbg(DBG_PRINT, "(GRADING3A 3.d)\n");

    vmarea_t* area = NULL;
    uint32_t endvfn = startvfn + npages;

    list_iterate_begin(&map->vmm_list, area, vmarea_t, vma_plink) {
        if (area->vma_start < endvfn && area->vma_end > startvfn)
        {
            dbg(DBG_PRINT, "(GRADING3B)\n");
            return 0;
        }
    } list_iterate_end();

    dbg(DBG_PRINT, "(GRADING3B)\n");
    return 1;
*/
NOT_YET_IMPLEMENTED("VM:vmmap rangeempty");
return 0;
}

/* Read into 'buf' from the virtual address space of 'map' starting at
 * 'vaddr' for size 'count'. To do so, you will want to find the vmareas
 * to read from, then find the pframes within those vmareas corresponding
 * to the virtual addresses you want to read, and then read from the
 * physical memory that pframe points to. You should not check permissions
 * of the areas. Assume (KASSERT) that all the areas you are accessing exist.
 * Returns 0 on success, -errno on error.
 */
int
vmmap_read(vmmap_t *map, const void *vaddr, void *buf, size_t count)
{
/*
    if (count == 0) return 0;
    if (vaddr == NULL) return -EFAULT;

    uintptr_t start_addr = (uintptr_t)vaddr;
    int start_vfn        = ADDR_TO_PN(start_addr);
    int start_offset     = PAGE_OFFSET(start_addr);

    uintptr_t end_addr   = start_addr + count - 1;
    int end_vfn          = ADDR_TO_PN(end_addr);

    if (start_vfn == end_vfn) {
        vmarea_t *vma = vmmap_lookup(map, start_vfn);
        if (!vma) return -EFAULT;

        pframe_t *pf = NULL;
        int ret = pframe_lookup(vma->vma_obj,
                                vma->vma_off + (start_vfn - vma->vma_start),
                                0,
                                &pf);
        if (ret != 0) return ret;

        memcpy(buf, (const char *)pf->pf_addr + start_offset, count);
        return 0;
    }

    size_t bytes_done   = 0;
    int    current_vfn  = start_vfn;

    while (current_vfn <= end_vfn) {
        vmarea_t *vma = vmmap_lookup(map, current_vfn);
        if (!vma) return -EFAULT;

        pframe_t *pf = NULL;
        int ret = pframe_lookup(vma->vma_obj,
                                vma->vma_off + (current_vfn - vma->vma_start),
                                0,
                                &pf);
        if (ret != 0) return ret;

        size_t copy_len;
        if (current_vfn == start_vfn) {
            copy_len = PAGE_SIZE - start_offset;
        } else if (current_vfn == end_vfn) {
            copy_len = count - bytes_done;
        } else {
            copy_len = PAGE_SIZE;
        }

        memcpy((char *)buf + bytes_done,
               (const char *)pf->pf_addr + ((current_vfn == start_vfn) ? start_offset : 0),
               copy_len);

        bytes_done   += copy_len;
        current_vfn  += 1;
    }

    return 0;
*/
NOT_YET_IMPLEMENTED("VM:vmmap read");
return 0;
}

/* Write from 'buf' into the virtual address space of 'map' starting at
 * 'vaddr' for size 'count'. To do this, you will need to find the correct
 * vmareas to write into, then find the correct pframes within those vmareas,
 * and finally write into the physical addresses that those pframes correspond
 * to. You should not check permissions of the areas you use. Assume (KASSERT)
 * that all the areas you are accessing exist. Remember to dirty pages!
 * Returns 0 on success, -errno on error.
 */
int
vmmap_write(vmmap_t *map, void *vaddr, const void *buf, size_t count)
{
/*
    if (count == 0) return 0;
    if (vaddr == NULL) return -EFAULT;

    uintptr_t start_addr = (uintptr_t)vaddr;
    int start_vfn        = ADDR_TO_PN(start_addr);
    int start_offset     = PAGE_OFFSET(start_addr);

    uintptr_t end_addr   = start_addr + count - 1;
    int end_vfn          = ADDR_TO_PN(end_addr);

    if (start_vfn == end_vfn) {
        vmarea_t *vma = vmmap_lookup(map, start_vfn);
        if (!vma) return -EFAULT;

        pframe_t *pf = NULL;
        int ret = pframe_lookup(vma->vma_obj,
                                vma->vma_off + (start_vfn - vma->vma_start),
                                1,
                                &pf);
        if (ret != 0) return ret;

        memcpy((char *)pf->pf_addr + start_offset, buf, count);
        pframe_dirty(pf);
        return 0;
    }

    size_t bytes_done   = 0;
    int    current_vfn  = start_vfn;

    while (current_vfn <= end_vfn) {
        vmarea_t *vma = vmmap_lookup(map, current_vfn);
        if (!vma) return -EFAULT;

        pframe_t *pf = NULL;
        int ret = pframe_lookup(vma->vma_obj,
                                vma->vma_off + (current_vfn - vma->vma_start),
                                1,
                                &pf);
        if (ret != 0) return ret;

        size_t copy_len;
        if (current_vfn == start_vfn) {
            copy_len = PAGE_SIZE - start_offset;
            memcpy((char *)pf->pf_addr + start_offset,
                   (const char *)buf + bytes_done,
                   copy_len);
        } else if (current_vfn == end_vfn) {
            copy_len = count - bytes_done;
            memcpy((char *)pf->pf_addr,
                   (const char *)buf + bytes_done,
                   copy_len);
        } else {
            copy_len = PAGE_SIZE;
            memcpy((char *)pf->pf_addr,
                   (const char *)buf + bytes_done,
                   copy_len);
        }

        pframe_dirty(pf);
        bytes_done  += copy_len;
        current_vfn += 1;
    }

    return 0;
*/
NOT_YET_IMPLEMENTED("VM:vmmap write");
return 0;
}
